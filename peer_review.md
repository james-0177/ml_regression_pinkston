## Peer Review:  Regression Analysis

### Name of Peer and link to Project File:  [Kellie Leopold](https://github.com/kjleopold/ml_regression_kjleopold/blob/main/ml_regression_kjleopold.ipynb)


1. Clarity & Organization (Is the notebook structured and easy to follow?)  **<span style="color: teal">The notebook is nicely structured and easy to follow. The numbered outline, use of bullet points, and clear separation of sections make the workflow very smooth. One area that could use more clarity is the feature engineering section. For example, Section 1 showed that there were no missing values, so the missing-value handling step in Section 2.2 wasnâ€™t necessary and could confuse readers. Adding a bit more explanation around why each feature engineering step was included would make the flow even stronger.</span>**

2. Feature Selection & Justification (Do the chosen features make sense given the objectives?)  **<span style="color: teal">Most of the chosen features make sense for predicting medical insurance charges. Age, BMI, number of children, and smoking status are clearly relevant, and the interaction term bmi_smoker is a reasonable way to capture how BMI and smoking together influence costs. However, some of the engineered features (bmi_charges and smoker_charges) incorporate the log-transformed target variable, which is a form of data leakage and could artificially inflate model performance. Including these features without explanation may confuse readers and raises concerns about the validity of the predictions. A clearer justification of why each interaction term was included, and which are safe versus leaky, would strengthen this section and make the feature selection process more transparent.</span>**

3. Model Performance & Comparisons (Are the results and comparisons clearly explained?)  **<span style="color: teal">The notebook clearly reports performance metrics (R^2, MAE, RMSE) for the Linear Regression model and both pipelines, and the comparison table makes it easy to see differences. The reflection explains why Pipeline 2 performs better due to Polynomial Features capturing more complex relationships. However, the extremely high R^2 for Pipeline 2 (0.999) is unrealistic for real-world prediction and is likely caused by data leakage from features that include the target variable. Additionally, the explanation could address potential overfitting, as R^2 near 1 often indicates that the model may not generalize beyond the training data.</span>**

4. Reflection Quality (Are insights well thought out?)  **<span style="color: teal">The notebook includes thoughtful reflections throughout, summarizing findings, challenges, and potential next steps. She clearly identifies age, BMI, and smoking as the strongest predictors and notes the value of interaction terms. The reflections also show learning about pipelines, scaling, and interpreting model outputs, which is valuable. However, the reflections do not mention the data leakage introduced by features that include the target variable, nor do they discuss potential overfitting, especially given the extremely high R^2 in Pipeline 2. Including these points would make the reflections more complete and help readers better understand the limitations and implications of the modeling choices.</span>**
